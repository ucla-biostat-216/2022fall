{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Orthogonal-projections\" data-toc-modified-id=\"Orthogonal-projections-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Orthogonal projections</a></div><div class=\"lev2 toc-item\"><a href=\"#Ortho-complementary-subspaces\" data-toc-modified-id=\"Ortho-complementary-subspaces-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Ortho-complementary subspaces</a></div><div class=\"lev2 toc-item\"><a href=\"#The-fundamental-theorem-of-linear-algebra\" data-toc-modified-id=\"The-fundamental-theorem-of-linear-algebra-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The fundamental theorem of linear algebra</a></div><div class=\"lev2 toc-item\"><a href=\"#Orthogonal-projections\" data-toc-modified-id=\"Orthogonal-projections-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Orthogonal projections</a></div><div class=\"lev2 toc-item\"><a href=\"#Orthogonal-projection-into-a-column-space\" data-toc-modified-id=\"Orthogonal-projection-into-a-column-space-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Orthogonal projection into a column space</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal projections\n",
    "\n",
    "**Highlights** In this lecture, we'll show (1) the **fundamental theorem of linear algebra**: \n",
    "\n",
    "<img src=\"../04-vecsp/four_fundamental_subspaces.png\" width=400 align=\"center\"/>\n",
    "\n",
    "and (2) any symmetric, idempotent matrix $\\mathbf{P}$ is the orthogonal projector onto $\\mathcal{C}(\\mathbf{P})$ (along $\\mathcal{C}(\\mathbf{P})^\\perp = \\mathcal{N}(\\mathbf{P}')$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct sum\n",
    "\n",
    "- The **sum of two vector spaces** $\\mathcal{S}_1$ and $\\mathcal{S}_2$ of same order is\n",
    "$$\n",
    "\\mathcal{S}_1 + \\mathcal{S}_2 = \\{\\mathbf{x}_1 + \\mathbf{x}_2: \\mathbf{x}_1 \\in \\mathcal{S}_1, \\mathbf{x}_2 \\in \\mathcal{S}_2\\}.\n",
    "$$\n",
    "\n",
    "    In HW4, we show that $\\mathcal{S}_1 + \\mathcal{S}_2$ is a vector space.\n",
    "\n",
    "- **Dimension of a sum of subspaces.** Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two subspaces in $\\mathbb{R}^n$. Then\n",
    "$$\n",
    "    \\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2) - \\text{dim}(\\mathcal{S}_1 \\cap \\mathcal{S}_2).\n",
    "$$\n",
    "\n",
    "    Recall that the sum of two vector spaces $\\mathcal{S}_1 + \\mathcal{S}_2$ is defined in HW4.\n",
    "\n",
    "    Proof (optional): Idea: extend a basis of $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ to ones for $\\mathcal{S}_1$ and $\\mathcal{S}_2$. See, e.g., BR p156-157.  \n",
    "    \n",
    "- Corollaries.  \n",
    "    1. **Subadditivity of dim function.** Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two subspaces in $\\mathbb{R}^n$. Then\n",
    "$$\n",
    "    \\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) \\le \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2).\n",
    "$$   \n",
    "    \n",
    "    This is an immediate corollary of the preceding theorem. In HW4, try to give a self-contained proof without using the preceding theorem.\n",
    "\n",
    "    2. **Subadditivity of rank function.** Let $\\mathbf{A}$ and $\\mathbf{B}$ be two matrices of the same order. Then  \n",
    "$$\n",
    "    \\text{rank}(\\mathbf{A} + \\mathbf{B}) \\le \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}).\n",
    "$$\n",
    "\n",
    "    Proof: First claim trivially follows from the previous result or your own proof in HW4. For the second claim. First note $\\mathcal{C}(\\mathbf{A} + \\mathbf{B}) \\subseteq \\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B})$ (why?). Then\n",
    "\\begin{eqnarray*}\n",
    "    & & \\text{rank}(\\mathbf{A} + \\mathbf{B}) \\\\\n",
    "    &=& \\text{dim}(\\mathcal{C}(\\mathbf{A} + \\mathbf{B})) \\quad \\text{(definition of rank)} \\\\\n",
    "    &\\le& \\text{dim}(\\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B})) \\quad \\text{(monotonicity of dim)} \\\\\n",
    "    &\\le& \\text{dim}(\\mathcal{C}(\\mathbf{A})) + \\text{dim}(\\mathcal{C}(\\mathbf{B})) \\quad \\text{(subadditivity of dim)} \\\\\n",
    "    &=& \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}). \\quad \\text{(definition of rank)}\n",
    "\\end{eqnarray*}\n",
    "The second inequality follows from the first claim.\n",
    "    \n",
    "    \n",
    "- Two subspaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$ in a vector space $\\mathcal{V}$ are said to be **complementary** whenever \n",
    "$$\n",
    "    \\mathcal{V} = \\mathcal{S}_1 + \\mathcal{S}_2 \\text{ and } \\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}.\n",
    "$$\n",
    "In such cases, we say $\\mathcal{V}$ is a **direct sum** of $\\mathcal{S}_1$ and $\\mathcal{S}_2$ and denote $\\mathcal{V} = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$. \n",
    "\n",
    "- TODO: visualize. $\\mathbb{R}^3 = \\text{a plane} \\oplus \\text{a line}$. \n",
    "\n",
    "- Let $\\mathcal{S}_1, \\mathcal{S}_2$ be two subspaces of same order and $\\mathcal{V} = \\mathcal{S}_1 + \\mathcal{S}_2$. Following statements are equivalent:\n",
    "    1. $\\mathcal{V} = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$. \n",
    "    2. $\\text{dim}(\\mathcal{V}) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2)$.  \n",
    "    3. Any vector $\\mathbf{x} \\in \\mathcal{V}$ can be **uniquely** represented as\n",
    "    $$\n",
    "        \\mathbf{x} = \\mathbf{x}_1 + \\mathbf{x}_2, \\text{ where } \\mathbf{x}_1 \\in \\mathbf{S}_1, \\mathbf{x}_2 \\in \\mathbf{S}_2.\n",
    "    $$\n",
    "    We will refer to this as the **unique representation** or **unique decomposition** property of direct sums.  \n",
    "    \n",
    "    Proof (optional): We show that $1 \\Rightarrow 2 \\Rightarrow 3 \\Rightarrow 1$.  \n",
    "    $1 \\Rightarrow 2$: By definition of direct sum, we know $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Thus $\\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2)$.  \n",
    "    $2 \\Rightarrow 3$: By 2, we know $\\text{dim}(\\mathcal{S}_1 \\cap \\mathcal{S}_2) = 0$ so $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Let $\\mathbf{x} \\in \\mathcal{S}_1 + \\mathcal{S}_2$ and assume $\\mathbf{x}$ can be decomposed in two ways: $\\mathbf{x} = \\mathbf{u}_1 + \\mathbf{u}_2 = \\mathbf{v}_1 + \\mathbf{v}_2$, where $\\mathbf{u}_1, \\mathbf{v}_1 \\in \\mathcal{S}_1$ and $\\mathbf{u}_2, \\mathbf{v}_2 \\in \\mathcal{S}_2$. Then $\\mathbf{u}_1 - \\mathbf{v}_1 = -(\\mathbf{u}_2 - \\mathbf{v}_2)$, indicating that the vectors $\\mathbf{u}_1 - \\mathbf{v}_1$ and $\\mathbf{u}_2 - \\mathbf{v}_2$ belong to both $\\mathcal{S}_1$ and $\\mathcal{S}_2$ and thus must be $\\mathbf{0}$. Therefore $\\mathbf{u}_1 = \\mathbf{v}_1$ and $\\mathbf{u}_2 = \\mathbf{v}_2$.  \n",
    "    $3 \\Rightarrow 1$: We only need to show $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Let $\\mathbf{x} \\in \\mathcal{S}_1 \\cap \\mathcal{S}_2$. Decompose $\\mathbf{x}$ in two ways: $\\mathbf{x} = \\mathbf{x} + \\mathbf{0} = \\mathbf{0} + \\mathbf{x}$. Then by the uniqueness part of 3, $\\mathbf{x}=\\mathbf{0}$. So the only possible element in $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ is $\\mathbf{0}$.\n",
    "    \n",
    "<img src=\"../04-vecsp/four_fundamental_subspaces.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- An immediate corollary of the previous result is, for a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, \n",
    "    1. $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "    2. $\\mathbb{R}^n = \\mathcal{C}(\\mathbf{A}') \\oplus \\mathcal{N}(\\mathbf{A})$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortho-complementary subspaces\n",
    "\n",
    "- An **orthocomplement set** of a set $\\mathcal{X}$ (not necessarily a subspace) in a vector space $\\mathcal{V} \\subseteq \\mathbb{R}^m$ is defined as\n",
    "$$\n",
    "    \\mathcal{X}^\\perp = \\{ \\mathbf{u} \\in \\mathcal{V}: \\langle \\mathbf{x}, \\mathbf{u} \\rangle = 0 \\text{ for all } \\mathbf{x} \\in \\mathcal{X}\\}.\n",
    "$$\n",
    "$\\mathcal{X}^\\perp$ is always a vector space regardless $\\mathcal{X}$ is a vector space or not.\n",
    "\n",
    "    Proof: exercise.\n",
    "\n",
    "- TODD: visualize $\\mathbb{R}^3 = \\text{a plane} \\oplus \\text{plan}^\\perp$.\n",
    "    \n",
    "- **Direct sum theorem for orthocomplementary subspaces.** Let $\\mathcal{S}$ be a subspace of a vector space $\\mathcal{V}$ with $\\text{dim}(\\mathcal{V}) = m$. Then the following statements are true.    \n",
    "    1. $\\mathcal{V} = \\mathcal{S} + \\mathcal{S}^\\perp$. That is every vector $\\mathbf{y} \\in \\mathcal{V}$ can be expressed as $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$, where $\\mathbf{u} \\in \\mathcal{S}$ and $\\mathbf{v} \\in \\mathcal{S}^\\perp$.  \n",
    "    2. $\\mathcal{S} \\cap \\mathcal{S}^\\perp = \\{\\mathbf{0}\\}$ (essentially disjoint).  \n",
    "    3. $\\mathcal{V} = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$.  \n",
    "    4. $m = \\text{dim}(\\mathcal{S}) + \\text{dim}(\\mathcal{S}^\\perp)$.  \n",
    "    By the uniqueness of decomposition for direct sum, we know the expression of $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$ is also **unique**.\n",
    "\n",
    "    Proof of 1: Let $\\{\\mathbf{z}_1, \\ldots, \\mathbf{z}_r\\}$ be an orthonormal basis of $\\mathcal{S}$ and extend it to an orthonormal basis $\\{\\mathbf{z}_1, \\ldots, \\mathbf{z}_r, \\mathbf{z}_{r+1}, \\ldots, \\mathbf{z}_m\\}$ of $\\mathcal{V}$. Then any $\\mathbf{y} \\in \\mathcal{V}$ can be expanded as\n",
    "$$\n",
    "    \\mathbf{y} = (\\alpha_1 \\mathbf{z}_1 + \\cdots + \\alpha_r \\mathbf{z}_r) + (\\alpha_{r+1} \\mathbf{z}_{r+1} + \\cdots + \\alpha_m \\mathbf{z}_m),\n",
    "$$\n",
    "where the first sum belongs to $\\mathcal{S}$ and the second to $\\mathcal{S}^\\perp$.  \n",
    "    Proof of 2: Suppose $\\mathbf{x} \\in \\mathcal{S} \\cap \\mathcal{S}^\\perp$, then $\\mathbf{x} \\perp \\mathbf{x}$, i.e., $\\langle \\mathbf{x}, \\mathbf{x} \\rangle = 0$. Therefore $\\mathbf{x} = \\mathbf{0}$.  \n",
    "    Proof of 3: Statement 1 says $\\mathcal{V} = \\mathcal{S} + \\mathcal{S}^\\perp$. Statement 2 says $\\mathcal{S}$ and $\\mathcal{S}^\\perp$ are essentially disjoint. Thus $\\mathcal{V} = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$.  \n",
    "    Proof of 4: Follows from essential disjointness between $\\mathcal{S}$ and $\\mathcal{S}^\\perp$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamental theorem of linear algebra\n",
    "\n",
    "- Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Then\n",
    "    1. $\\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}')$ and $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "    2. $\\mathcal{C}(\\mathbf{A}) = \\mathcal{N}(\\mathbf{A}')^\\perp$.  \n",
    "    3. $\\mathcal{N}(\\mathbf{A})^\\perp = \\mathcal{C}(\\mathbf{A}')$ and $\\mathbb{R}^n = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A}')$.  \n",
    "    \n",
    "    Proof of 1: To show $\\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}')$,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{x} \\in \\mathcal{N}(\\mathbf{A}') \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{A}' \\mathbf{x} = \\mathbf{0} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} \\text{ is orthogonal to columns of } \\mathbf{A} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} \\in \\mathcal{C}(\\mathbf{A})^\\perp.\n",
    "\\end{eqnarray*}\n",
    "Then, $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "\n",
    "    Proof of 2: Since $\\mathcal{C}(\\mathbf{A})$ is a subspace, $(\\mathcal{C}(\\mathbf{A})^\\perp)^\\perp = \\mathcal{N}(\\mathbf{A}')^\\perp$. \n",
    "    \n",
    "    Proof of 3: Applying part 2 to $\\mathbf{A}'$, we have\n",
    "$$\n",
    "    \\mathcal{C}(\\mathbf{A}') = \\mathcal{N}((\\mathbf{A}')')^\\perp = \\mathcal{N}(\\mathbf{A})^\\perp\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\mathbb{R}^n = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A}').\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "<img src=\"../06-matinv/three_projs.png\" width=600 align=\"center\"/>\n",
    "\n",
    "- Suppose $\\mathcal{S}$ is a subspace of a vector space $\\mathcal{V}$ and $\\mathbf{y} \\in \\mathcal{V}$. Let $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$ be the unique representation of $\\mathbf{y}$ with $\\mathbf{u} \\in \\mathcal{S}$ and $\\mathbf{v} \\in \\mathcal{S}^\\perp$. Then the vector $\\mathbf{u}$ is called the **orthogonal projection** of $\\mathbf{y}$ into $\\mathcal{S}$ (along $\\mathcal{S}^\\perp$).\n",
    "\n",
    "- **The closest point theorem.** Let $\\mathcal{S}$ be a subspace of some vector space $\\mathcal{V}$ and $\\mathbf{y} \\in \\mathcal{V}$. The orthogonal projection of $\\mathbf{y}$ into $\\mathcal{S}$ is the **unique** point in $\\mathcal{S}$ that is closest to $\\mathbf{y}$. In other words, if $\\mathbf{u}$ is the orthogonal projection of $\\mathbf{y}$ into $\\mathcal{S}$, then\n",
    "$$\n",
    "    \\|\\mathbf{y} - \\mathbf{u}\\|^2 \\le \\|\\mathbf{y} - \\mathbf{w}\\|^2 \\text{ for all } \\mathbf{w} \\in \\mathcal{S},\n",
    "$$\n",
    "with equality holding only when $\\mathbf{w} = \\mathbf{u}$. \n",
    "\n",
    "    Proof: Picture. \n",
    "\\begin{eqnarray*}    \n",
    "& & \\|\\mathbf{y} - \\mathbf{w}\\|^2 \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{u} + \\mathbf{u} - \\mathbf{w}\\|^2 \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{u}\\|^2 + 2(\\mathbf{y} - \\mathbf{u})'(\\mathbf{u} - \\mathbf{w}) +  \\|\\mathbf{u} - \\mathbf{w}\\|^2 \\quad \\quad \\quad (\\mathbf{u} - \\mathbf{w} \\in \\mathcal{S}, \\mathbf{y} - \\mathbf{u} \\in \\mathcal{S}^\\perp) \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{u}\\|^2 + \\|\\mathbf{u} - \\mathbf{w}\\|^2 \\\\\n",
    "&\\ge& \\|\\mathbf{y} - \\mathbf{u}\\|^2.\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "- Let $\\mathbb{R}^n = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$. A square matrix $\\mathbf{P}_{\\mathcal{S}}$ is called the **orthogonal porjector** into $\\mathcal{S}$ if, for every $\\mathbf{y} \\in \\mathbb{R}^n$, $\\mathbf{P}_{\\mathcal{S}} \\mathbf{y}$ is the projection of $\\mathbf{y}$ into $\\mathcal{S}$ along $\\mathcal{S}^\\perp$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projection into a column space\n",
    "\n",
    "- For a matrix $\\mathbf{X}$, the orthogonal projector onto $\\mathcal{C}(\\mathbf{X})$ is written as $\\mathbf{P}_{\\mathbf{X}}$.  \n",
    "\n",
    "<img src=\"./ls_projection.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- Let $\\mathbf{y} \\in \\mathbb{R}^n$ and $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$. \n",
    "    1. The orthogonal projector of $\\mathbf{y}$ into $\\mathcal{C}(\\mathbf{X})$ is given by $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta}$, where $\\boldsymbol{\\beta}$ satisfies the **normal equation**\n",
    "$$\n",
    "    \\mathbf{X}' \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "Normal equation always has solution(s) (why?) and any generalized inverse $(\\mathbf{X}'\\mathbf{X})^-$ yields a solution $\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^- \\mathbf{X}' \\mathbf{y}$. Therefore,\n",
    "$$\n",
    "\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^- \\mathbf{X}'.\n",
    "$$\n",
    "for any generalized inverse $(\\mathbf{X}' \\mathbf{X})^-$.\n",
    "    2. If $\\mathbf{X}$ has full column rank, then the orthogonal projector into $\\mathcal{C}(\\mathbf{X})$ is given by\n",
    "$$\n",
    "    \\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'.\n",
    "$$\n",
    "\n",
    "    Proof of 1: Since the orthogonal projection of $\\mathbf{y}$ into $\\mathcal{C}(\\mathbf{X})$ lives in $\\mathcal{C}(\\mathbf{X})$, thus can be written as $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta}$ for some $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$. Furthermore, $\\mathbf{v} = \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\in \\mathcal{C}(\\mathbf{X})^\\perp$ is orthogonal to any vectors in $\\mathcal{C}(\\mathbf{X})$ including the columns of $\\mathbf{X}$. Thus\n",
    "$$\n",
    "    \\mathbf{X}' (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\mathbf{0},\n",
    "$$\n",
    "or equivalently,\n",
    "$$\n",
    "    \\mathbf{X}' \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "    Proof of 2: If $\\mathbf{X}$ has full column rank, $\\mathbf{X}' \\mathbf{X}$ is non-singular and the solution to the normal equation is uniquely determined by $\\boldsymbol{\\beta} = (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y}$, and the orthogonal projection is $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y}$. \n",
    "\n",
    "- **Uniqueness of orthogonal projector.** Let $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{n \\times p}$, both of full column rank and $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{B})$. Then $\\mathbf{P}_{\\mathbf{A}} = \\mathbf{P}_{\\mathbf{B}}$. \n",
    "\n",
    "    Proof: Since $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{B})$, there exists a non-singular $\\mathbf{C} \\in \\mathbb{R}^{p \\times p}$ such that $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$. Then\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{P}_{\\mathbf{A}} &=& \\mathbf{A} (\\mathbf{A}' \\mathbf{A})^{-1} \\mathbf{A}' \\\\\n",
    "    &=& \\mathbf{B} \\mathbf{C} (\\mathbf{C}' \\mathbf{B}' \\mathbf{B} \\mathbf{C})^{-1} \\mathbf{C}' \\mathbf{B}' \\\\\n",
    "    &=& \\mathbf{B} \\mathbf{C} \\mathbf{C}^{-1} (\\mathbf{B}' \\mathbf{B})^{-1} (\\mathbf{C}')^{-1} \\mathbf{C}' \\mathbf{B}' \\\\\n",
    "    &=&  \\mathbf{B} (\\mathbf{B}' \\mathbf{B})^{-1} \\mathbf{B}' \\\\\n",
    "    &=& \\mathbf{P}_{\\mathbf{B}}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Let $\\mathbf{P}_\\mathbf{X}$ be the orthogonal projector into $\\mathcal{C}(\\mathbf{X})$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ has full column rank. Following statements are true.  \n",
    "    1. $\\mathbf{P}_\\mathbf{X}$ and $\\mathbf{I} - \\mathbf{P}_\\mathbf{X}$ are both symmetric and idemponent. \n",
    "    2. $\\mathbf{P}_\\mathbf{X} \\mathbf{X} = \\mathbf{X}$ and $(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{X} = \\mathbf{O}$.  \n",
    "    3. $\\mathcal{C}(\\mathbf{X}) = \\mathcal{C}(\\mathbf{P}_\\mathbf{X})$ and $\\text{rank}(\\mathbf{P}_\\mathbf{X}) = \\text{rank}(\\mathbf{X}) = p$.  \n",
    "    4. $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) = \\mathcal{N}(\\mathbf{X}') = \\mathcal{C}(\\mathbf{X})^\\perp$.  \n",
    "    5. $\\mathbf{I} - \\mathbf{P}_\\mathbf{X}$ is the orthogonal projector into $\\mathcal{N}(\\mathbf{X}')$ (or $\\mathcal{C}(\\mathbf{X})^\\perp)$.  \n",
    "    \n",
    "    Proof of 1: Check directly using $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$.  \n",
    "    \n",
    "    Proof of 2: Check directly using $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$. \n",
    "    \n",
    "    Proof of 3: Since $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$, \n",
    "$$\n",
    "    \\mathcal{C}(\\mathbf{P}_\\mathbf{X}) \\subseteq \\mathcal{C}(\\mathbf{X}) = \\mathcal{C}(\\mathbf{P}_\\mathbf{X} \\mathbf{X}) \\subseteq \\mathcal{C}(\\mathbf{P}_\\mathbf{X}). \n",
    "$$\n",
    "\n",
    "    Proof of 4 (optional): The second equality is simply the fundamental theorem of linear algebra. For the first equality, first we show $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\subseteq \\mathcal{N}(\\mathbf{X}')$:\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{u} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} = (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{v} \\text{ for some } \\mathbf{v} \\\\\n",
    "    &\\Rightarrow& \\mathbf{X}' \\mathbf{u} = [(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{X}]' \\mathbf{v} = \\mathbf{O} \\mathbf{v} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} \\in \\mathcal{N}(\\mathbf{X}').\n",
    "\\end{eqnarray*}\n",
    "To show the other direction $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\supseteq \\mathcal{N}(\\mathbf{X}')$,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{u} \\in \\mathcal{N}(\\mathbf{X}') \\\\\n",
    "    &\\Rightarrow& \\mathbf{X}' \\mathbf{u} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{P}_\\mathbf{X} \\mathbf{u} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{u} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} = \\mathbf{u} - \\mathbf{P}_\\mathbf{X} \\mathbf{u} = (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{u} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}).\n",
    "\\end{eqnarray*}    \n",
    "\n",
    "    Proof of 5: For any $\\mathbf{y} \\in \\mathbb{R}^n$, write $\\mathbf{y} = \\mathbf{P}_\\mathbf{X} \\mathbf{y} + (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{y}$.\n",
    "    \n",
    "- Let $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ be symmetric with rank $r$ and $\\mathbf{P} = \\mathbf{U} \\mathbf{U}'$ be a rank factorization of $\\mathbf{P}$. Then $\\mathbf{P}$ is an orthogonal projector in $\\mathcal{C}(\\mathbf{U})$ if and only if $\\mathbf{U}' \\mathbf{U} = \\mathbf{I}_r$.\n",
    "\n",
    "- Above result gives another way to construct an orthogonal projector into $\\mathcal{C}(\\mathbf{A})$, where $\\mathbf{A}$ might not be full column rank. Obtain an orthornormal basis $\\mathbf{Q} \\in \\mathbb{R}^{n \\times r}$ of $\\mathcal{C}(\\mathbf{A})$, e.g., by Gram-Schmidt, and let $\\mathbf{P} = \\mathbf{Q} \\mathbf{Q}'$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "134.716px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "412.712px",
    "left": "0px",
    "right": "1400.74px",
    "top": "32.2857px",
    "width": "133.725px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Projection-and-idempotent-matrix\" data-toc-modified-id=\"Projection-and-idempotent-matrix-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Projection and idempotent matrix</a></div><div class=\"lev2 toc-item\"><a href=\"#Direct-sum\" data-toc-modified-id=\"Direct-sum-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Direct sum</a></div><div class=\"lev2 toc-item\"><a href=\"#Projection\" data-toc-modified-id=\"Projection-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Projection</a></div><div class=\"lev2 toc-item\"><a href=\"#Cochran-theorem-(optional)\" data-toc-modified-id=\"Cochran-theorem-(optional)-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Cochran theorem (optional)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection and idempotent matrix\n",
    "\n",
    "**Highlights:** In this lecture, we learn that (1) $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$, $\\mathbb{R}^n = \\mathcal{C}(\\mathbf{A}') \\oplus \\mathcal{N}(\\mathbf{A})$; (2) any idempotent matrix $\\mathbf{P}$ is a projector into $\\mathcal{C}(\\mathbf{P})$ along $\\mathcal{N}(\\mathbf{P}) = \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$; (3) Given $\\mathbb{R}^n = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$, know how to construct a projector $\\mathbf{P}$ that projects into $\\mathcal{S}_1$ along $\\mathcal{S}_2$.\n",
    "\n",
    "Note when many authors say **projection** or **projectors**, they actually mean **orthogonal projection** or **orthogonal projectors**, which are subject of next lecture. In this course, we differentiate (oblique) projection and orthogonal projection. \n",
    "\n",
    "<img src=\"../06-matinv/three_projs.png\" width=600 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct sum\n",
    "\n",
    "- The **sum of two vector spaces** $\\mathcal{S}_1$ and $\\mathcal{S}_2$ of same order is\n",
    "$$\n",
    "\\mathcal{S}_1 + \\mathcal{S}_2 = \\{\\mathbf{x}_1 + \\mathbf{x}_2: \\mathbf{x}_1 \\in \\mathcal{S}_1, \\mathbf{x}_2 \\in \\mathcal{S}_2\\}.\n",
    "$$\n",
    "\n",
    "    In HW4, we showed that $\\mathcal{S}_1 + \\mathcal{S}_2$ is a vector space.\n",
    "\n",
    "- **Dimension of a sum of subspaces.** Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two subspaces in $\\mathbb{R}^n$. Then\n",
    "$$\n",
    "    \\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2) - \\text{dim}(\\mathcal{S}_1 \\cap \\mathcal{S}_2).\n",
    "$$\n",
    "\n",
    "    Recall that the sum of two vector spaces $\\mathcal{S}_1 + \\mathcal{S}_2$ is defined in HW4.\n",
    "\n",
    "    Proof (optional): Idea: extend a basis of $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ to ones for $\\mathcal{S}_1$ and $\\mathcal{S}_2$. See, e.g., BR p156-157.  \n",
    "    \n",
    "- Corollaries.  \n",
    "    1. **Subadditivity of dim function.** Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two subspaces in $\\mathbb{R}^n$. Then\n",
    "$$\n",
    "    \\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) \\le \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2).\n",
    "$$   \n",
    "    \n",
    "    This is an immediate corollary of the preceding theorem. In HW4, try to give a self-contained proof without using the preceding theorem.\n",
    "\n",
    "    2. **Subadditivity of rank function.** Let $\\mathbf{A}$ and $\\mathbf{B}$ be two matrices of the same order. Then  \n",
    "$$\n",
    "    \\text{rank}(\\mathbf{A} + \\mathbf{B}) \\le \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}).\n",
    "$$\n",
    "\n",
    "    Proof: First claim trivially follows from the previous result or your own proof in HW4. For the second claim. First note $\\mathcal{C}(\\mathbf{A} + \\mathbf{B}) \\subseteq \\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B})$ (why?). Then\n",
    "\\begin{eqnarray*}\n",
    "    & & \\text{rank}(\\mathbf{A} + \\mathbf{B}) \\\\\n",
    "    &=& \\text{dim}(\\mathcal{C}(\\mathbf{A} + \\mathbf{B})) \\quad \\text{(definition of rank)} \\\\\n",
    "    &\\le& \\text{dim}(\\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B})) \\quad \\text{(monotonicity of dim)} \\\\\n",
    "    &\\le& \\text{dim}(\\mathcal{C}(\\mathbf{A})) + \\text{dim}(\\mathcal{C}(\\mathbf{B})) \\quad \\text{(subadditivity of dim)} \\\\\n",
    "    &=& \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}). \\quad \\text{(definition of rank)}\n",
    "\\end{eqnarray*}\n",
    "The second inequality follows from the first claim.\n",
    "    \n",
    "    \n",
    "- Two subspaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$ in a vector space $\\mathcal{V}$ are said to be **complementary** whenever \n",
    "$$\n",
    "    \\mathcal{V} = \\mathcal{S}_1 + \\mathcal{S}_2 \\text{ and } \\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}.\n",
    "$$\n",
    "In such cases, we say $\\mathcal{V}$ is a **direct sum** of $\\mathcal{S}_1$ and $\\mathcal{S}_2$ and denote $\\mathcal{V} = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$. \n",
    "\n",
    "- TODO: visualize. $\\mathbb{R}^3 = \\text{a plane} \\oplus \\text{a line}$. \n",
    "\n",
    "- Let $\\mathcal{S}_1, \\mathcal{S}_2$ be two subspaces of same order and $\\mathcal{V} = \\mathcal{S}_1 + \\mathcal{S}_2$. Following statements are equivalent:\n",
    "    1. $\\mathcal{V} = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$. \n",
    "    2. $\\text{dim}(\\mathcal{V}) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2)$.  \n",
    "    3. Any vector $\\mathbf{x} \\in \\mathcal{V}$ can be **uniquely** represented as\n",
    "    $$\n",
    "        \\mathbf{x} = \\mathbf{x}_1 + \\mathbf{x}_2, \\text{ where } \\mathbf{x}_1 \\in \\mathbf{S}_1, \\mathbf{x}_2 \\in \\mathbf{S}_2.\n",
    "    $$\n",
    "    We will refer to this as the **unique representation** or **unique decomposition** property of direct sums.  \n",
    "    \n",
    "    Proof (optional): We show that $1 \\Rightarrow 2 \\Rightarrow 3 \\Rightarrow 1$.  \n",
    "    $1 \\Rightarrow 2$: By definition of direct sum, we know $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Thus $\\text{dim}(\\mathcal{S}_1 + \\mathcal{S}_2) = \\text{dim}(\\mathcal{S}_1) + \\text{dim}(\\mathcal{S}_2)$.  \n",
    "    $2 \\Rightarrow 3$: By 2, we know $\\text{dim}(\\mathcal{S}_1 \\cap \\mathcal{S}_2) = 0$ so $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Let $\\mathbf{x} \\in \\mathcal{S}_1 + \\mathcal{S}_2$ and assume $\\mathbf{x}$ can be decomposed in two ways: $\\mathbf{x} = \\mathbf{u}_1 + \\mathbf{u}_2 = \\mathbf{v}_1 + \\mathbf{v}_2$, where $\\mathbf{u}_1, \\mathbf{v}_1 \\in \\mathcal{S}_1$ and $\\mathbf{u}_2, \\mathbf{v}_2 \\in \\mathcal{S}_2$. Then $\\mathbf{u}_1 - \\mathbf{v}_1 = -(\\mathbf{u}_2 - \\mathbf{v}_2)$, indicating that the vectors $\\mathbf{u}_1 - \\mathbf{v}_1$ and $\\mathbf{u}_2 - \\mathbf{v}_2$ belong to both $\\mathcal{S}_1$ and $\\mathcal{S}_2$ and thus must be $\\mathbf{0}$. Therefore $\\mathbf{u}_1 = \\mathbf{v}_1$ and $\\mathbf{u}_2 = \\mathbf{v}_2$.  \n",
    "    $3 \\Rightarrow 1$: We only need to show $\\mathcal{S}_1 \\cap \\mathcal{S}_2 = \\{\\mathbf{0}\\}$. Let $\\mathbf{x} \\in \\mathcal{S}_1 \\cap \\mathcal{S}_2$. Decompose $\\mathbf{x}$ in two ways: $\\mathbf{x} = \\mathbf{x} + \\mathbf{0} = \\mathbf{0} + \\mathbf{x}$. Then by the uniqueness part of 3, $\\mathbf{x}=\\mathbf{0}$. So the only possible element in $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ is $\\mathbf{0}$.\n",
    "    \n",
    "<img src=\"../04-vecsp/four_fundamental_subspaces.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- An immediate corollary of the previous result is, for a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, \n",
    "    1. $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "    2. $\\mathbb{R}^n = \\mathcal{C}(\\mathbf{A}') \\oplus \\mathcal{N}(\\mathbf{A})$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "- Let $\\mathbb{R}^n = \\mathcal{S} \\oplus \\mathcal{T}$ and $\\mathbf{y} = \\mathbf{x} + \\mathbf{z}$ be the unique representation of $\\mathbf{y} \\in \\mathbb{R}^n$ with $\\mathbf{x} \\in \\mathcal{S}$ and $\\mathbf{z} \\in \\mathcal{T}$. Then\n",
    "    - the vector $\\mathbf{x}$ is called the **projection** of $\\mathbf{y}$ into $\\mathcal{S}$ along $\\mathcal{T}$;\n",
    "    - the vector $\\mathbf{z}$ is called the **projection** of $\\mathbf{y}$ into $\\mathcal{T}$ along $\\mathcal{S}$.\n",
    "\n",
    "- Let $\\mathbb{R}^n = \\mathcal{S} \\oplus \\mathcal{T}$. We call a matrix $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ a **projection matrix** or **projector** into $\\mathcal{S}$ along $\\mathcal{T}$ if, for any $\\mathbf{y} \\in \\mathbb{R}^n$, $\\mathbf{P} \\mathbf{y}$ is the projection of $\\mathbf{y}$ into $\\mathcal{S}$ along $\\mathcal{T}$.\n",
    "\n",
    "- If $\\mathbf{P}$ is a projector into $\\mathcal{S}$ along $\\mathcal{T}$, then \n",
    "    1. $\\mathbf{P} \\mathbf{x} = \\mathbf{x}$ for all $\\mathbf{x} \\in \\mathcal{S}$.  \n",
    "    2. $\\mathbf{P} \\mathbf{z} = \\mathbf{0}$ for all $\\mathbf{z} \\in \\mathcal{T}$.  \n",
    "\n",
    "    Proof: For 1, since $\\mathbf{x} = \\mathbf{x} + \\mathbf{0}$, where $\\mathbf{x} \\in \\mathcal{S}$ and $\\mathbf{0} \\in \\mathcal{T}$. By uniqueness of the projection, $\\mathbf{P} \\mathbf{x}$ must be equal to $\\mathbf{x}$. For 2, $\\mathbf{z} = \\mathbf{0} + \\mathbf{z}$ is the unique decomposition. Then $\\mathbf{P} \\mathbf{z}$ must be $\\mathbf{0}$.\n",
    "\n",
    "- A square matrix $\\mathbf{P}$ is said to be **idempotent** if $\\mathbf{P}^2 = \\mathbf{P}$.\n",
    "\n",
    "- Following statements about a square matrix $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ are equivalent:\n",
    "    1. $\\mathbf{P}$ is a projector.  \n",
    "    2. $\\mathbf{P}$ and $\\mathbf{I} - \\mathbf{P}$ are idemponent. That is $\\mathbf{P}^2 = \\mathbf{P}$ and $(\\mathbf{I} - \\mathbf{P})^2 = \\mathbf{I} - \\mathbf{P}$.  \n",
    "    3. $\\mathcal{N}(\\mathbf{P}) = \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$.  \n",
    "    4. $\\text{rank}(\\mathbf{P}) + \\text{rank}(\\mathbf{I} - \\mathbf{P}) = n$.  \n",
    "    5. $\\mathbb{R}^n = \\mathcal{C}(\\mathbf{P}) \\oplus \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$.  \n",
    "**We can use any of these five statements as a definition of a projector.**  \n",
    "    In words, any idempotent matrix $\\mathbf{P}$ projects into $\\mathcal{C}(\\mathbf{P})$ along $\\mathcal{N}(\\mathbf{P})$ and $\\mathbf{I} - \\mathbf{P}$ projects into $\\mathcal{N}(\\mathbf{P})$ along $\\mathcal{C}(\\mathbf{P})$.\n",
    "    \n",
    "    Proof (optional): We show $1 \\Rightarrow 2 \\Rightarrow 3 \\Rightarrow 4 \\Rightarrow 5 \\Rightarrow 1$.  \n",
    "    $1 \\Rightarrow 2$: Suppose $\\mathbf{P}$ is a projector into $\\mathcal{S}$ along $\\mathcal{T}$, where $\\mathcal{S} \\oplus \\mathcal{T} = \\mathbb{R}^n$. Then $\\mathbf{P}^2 \\mathbf{y} = \\mathbf{P}(\\mathbf{P} \\mathbf{y}) = \\mathbf{P} \\mathbf{y}$ for any $\\mathbf{y} \\in \\mathbb{R}^n$. Now taking $\\mathbf{y} = \\mathbf{e}_1, \\ldots, \\mathbf{e}_n$, we showed each column of $\\mathbf{P}^2$ is equal to the corresponding column of $\\mathbf{P}$. Therefore $\\mathbf{P}^2 = \\mathbf{P}$. Finally $(\\mathbf{I} - \\mathbf{P})^2 = \\mathbf{I} - 2\\mathbf{P} + \\mathbf{P}^2 = \\mathbf{I} - \\mathbf{P}$.  \n",
    "    $2 \\Rightarrow 3$: To show $\\mathcal{N}(\\mathbf{P}) \\supseteq \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$, \n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{x} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}) \\\\\n",
    "    &\\Rightarrow& \\mathbf{x} = (\\mathbf{I} - \\mathbf{P}) \\mathbf{v} \\text{ for some } \\mathbf{v} \\\\\n",
    "    &\\Rightarrow& \\mathbf{P} \\mathbf{x} = \\mathbf{P} (\\mathbf{I} - \\mathbf{P}) \\mathbf{v} = (\\mathbf{P} - \\mathbf{P}^2) \\mathbf{v} = \\mathbf{0}_{n \\times n} \\mathbf{v} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{x} \\in \\mathcal{N}(\\mathbf{P}).\n",
    "\\end{eqnarray*}\n",
    "To show $\\mathcal{N}(\\mathbf{P}) \\subseteq \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{x} \\in \\mathcal{N}(\\mathbf{P}) \\\\\n",
    "    &\\Rightarrow& \\mathbf{P} \\mathbf{x} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{x} = \\mathbf{x} - \\mathbf{P} \\mathbf{x} = (\\mathbf{I} - \\mathbf{P}) \\mathbf{x} \\\\\n",
    "    &\\Rightarrow& \\mathbf{x} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}).\n",
    "\\end{eqnarray*}\n",
    "    $3 \\Rightarrow 4$: By the Rank-Nullity theorem,\n",
    "$$\n",
    "    \\text{rank}(\\mathbf{P}) = n - \\text{nullity}(\\mathbf{P}) = n - \\text{dim}(\\mathcal{N}(\\mathbf{P})) = n - \\text{dim}(\\mathcal{C}(\\mathbf{I} - \\mathbf{P})) = n - \\text{rank}(\\mathcal{C}(\\mathbf{I} - \\mathbf{P})).\n",
    "$$  \n",
    "    $4 \\Rightarrow 5$: For any $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{x} = (\\mathbf{P} + \\mathbf{I} - \\mathbf{P}) \\mathbf{x}  = \\mathbf{P} \\mathbf{x} + (\\mathbf{I} - \\mathbf{P}) \\mathbf{x}$. Thus $\\mathbb{R}^n = \\mathcal{C}(\\mathbf{P}) + \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$. Then $\\mathcal{C}(\\mathbf{P})$ and $\\mathcal{C}(\\mathbf{I} - \\mathbf{P})$ must be a direct sum by earlier result (3 equivalent properties of direct sum).  \n",
    "    $5 \\Rightarrow 1$: For any $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{x} = (\\mathbf{P} + \\mathbf{I} - \\mathbf{P}) \\mathbf{x}  = \\mathbf{P} \\mathbf{x} + (\\mathbf{I} - \\mathbf{P}) \\mathbf{x}$, where $\\mathbf{P} \\mathbf{x} \\in \\mathcal{C}(\\mathbf{P})$ and $(\\mathbf{I} - \\mathbf{P}) \\mathbf{x} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P})$. 5 then confirms that $\\mathbf{P}$ projects into $\\mathcal{C}(\\mathbf{P})$ along $\\mathcal{C}(\\mathbf{I} - \\mathbf{P})$. \n",
    "    \n",
    "- Let $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ has rank $r$ and $\\mathbf{P} = \\mathbf{C} \\mathbf{R}$ be a rank factorization of $\\mathbf{P}$. Then $\\mathbf{P}$ is a projector if and only if $\\mathbf{R} \\mathbf{C} = \\mathbf{I}_r$.\n",
    "\n",
    "    Proof: Since $\\mathbf{P} = \\mathbf{C} \\mathbf{R}$ is a rank factorization, we have $\\mathbf{C} \\in \\mathbb{R}^{n \\times r}$ and $\\mathbf{R} \\in \\mathbb{R}^{r \\times n}$.  \n",
    "    If $\\mathbf{R} \\mathbf{C} = \\mathbf{I}_r$, then\n",
    "$$\n",
    "    \\mathbf{P}^2 = \\mathbf{C} \\mathbf{R} \\mathbf{C} \\mathbf{R} = \\mathbf{C} \\mathbf{I}_r \\mathbf{R} = \\mathbf{C} \\mathbf{R} = \\mathbf{P}.\n",
    "$$\n",
    "    If $\\mathbf{P}$ is a projector, then $\\mathbf{P}$ is idemponent. That is\n",
    "$$\n",
    "    \\mathbf{C} \\mathbf{R} \\mathbf{C} \\mathbf{R} = \\mathbf{C} \\mathbf{R}.\n",
    "$$\n",
    "Since $\\mathbf{P} = \\mathbf{C} \\mathbf{R}$ is a rank factorization, $\\text{rank}(\\mathbf{C}) = \\text{rank}(\\mathbf{P}) = r$. $\\mathbf{C}$ has full column rank (rows of $\\mathbf{C}$ span $\\mathbb{R}^r$); thus there exists a left inverse $\\mathbf{C}_L$ such that $\\mathbf{C}_L \\mathbf{C} = \\mathbf{I}_r$. Similarly $\\mathbf{R}$ having full row rank (columns of $\\mathbf{R}$ span $\\mathbb{R}^r$) implies that there exists a right inverse $\\mathbf{R}_R$ such that $\\mathbf{R} \\mathbf{R}_R = \\mathbf{I}_r$. Pre-multiplying by $\\mathbf{C}_L$ and post-multiplying by $\\mathbf{R}_R$ on both sides of above equation\n",
    "$$\n",
    "    \\mathbf{C}_L \\mathbf{C} \\mathbf{R} \\mathbf{C} \\mathbf{R} \\mathbf{R}_R = \\mathbf{C}_L \\mathbf{C} \\mathbf{R} \\mathbf{R}_R.\n",
    "$$\n",
    "gives the desired identity $\\mathbf{R}\\mathbf{C} = \\mathbf{I}_r$.\n",
    "\n",
    "- Above result gives us a recipe to **construct (oblique) projectors**. Suppose $\\mathbf{R}^n = \\mathcal{S}_1 \\oplus \\mathcal{S}_2$. How to construct a projector $\\mathbf{P}$ that projects into $\\mathcal{S}_1$ along $\\mathcal{S}_2$?\n",
    "\n",
    "    Solution: Let columns of $\\mathbf{B}_1 \\in \\mathbb{R}^{n \\times r}$ be a basis of $\\mathcal{S}_1$ and columns of $\\mathbf{B}_2 \\in \\mathbb{R}^{n \\times (n-r)}$ be a basis of $\\mathcal{S}_2$. Let $\\mathbf{P} = \\mathbf{B}_1 \\mathbf{R}$ is a rank factorization of the desired projector. Then $\\mathbf{R} \\mathbf{B}_1 = \\mathbf{I}_r$ and $\\mathbf{R} \\mathbf{B}_2 = \\mathbf{0}_{r \\times (n-r)}$. That is\n",
    "$$\n",
    "\\mathbf{R} (\\mathbf{B}_1 \\,\\, \\mathbf{B}_2) = (\\mathbf{I}_r \\,\\, \\mathbf{0}_{r \\times (n-r)}).\n",
    "$$\n",
    "We get $\\mathbf{R} = (\\mathbf{I}_r \\,\\, \\mathbf{0}_{r \\times (n-r)}) (\\mathbf{B}_1 \\,\\, \\mathbf{B}_2)^{-1}$.\n",
    "\n",
    "- Let $\\mathbf{P}$ be a projector, or equivalently idempotent. Then $\\text{tr}(\\mathbf{P}) = \\text{rank}(\\mathbf{P})$.  \n",
    "\n",
    "    Proof: By HW2 (BV 10.11),\n",
    "$$\n",
    "    \\text{tr}(\\mathbf{P}) = \\text{tr}(\\mathbf{C} \\mathbf{R}) = \\text{tr}(\\mathbf{R} \\mathbf{C}) = \\text{tr}(\\mathbf{I}_r) = r = \\text{rank}(\\mathbf{P}).\n",
    "$$\n",
    "\n",
    "- Example: Homework 5. Find the 3 projectors in the picture. \n",
    "\n",
    "<img src=\"../06-matinv/three_projs.png\" width=600 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "n = 5\n",
    "# this P is symmetric, thus indeed an orthogonal projector\n",
    "P = (1/n) * ones(n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2\n",
       " 0.2  0.2  0.2  0.2  0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idempotent\n",
    "P^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(P), rank(I - P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a non-orthogonal projection: P = c * ones(n) * v'\n",
    "using Random\n",
    "Random.seed!(216)\n",
    "v = randn(n)\n",
    "c = 1 / sum(v)\n",
    "P = c * ones(n) * v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362\n",
       " 0.892643  1.00408  0.170126  0.226762  -1.29362"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(P), rank(I - P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cochran theorem (optional)\n",
    "\n",
    "This theorem is important for deriving F test in linear models.\n",
    "\n",
    "- Let $\\mathbf{P}_1, \\ldots, \\mathbf{P}_k$ be $n \\times n$ matrices with $\\sum_{i=1}^k \\mathbf{P}_i = \\mathbf{I}_n$. Then the following statements are equivalent:  \n",
    "    1. $\\sum_{i=1}^k \\text{rank}(\\mathbf{P}_i) = n$.  \n",
    "    2. $\\mathbf{P}_i \\mathbf{P}_j = \\mathbf{0}_{n \\times n}$ for $i \\ne j$.  \n",
    "    3. $\\mathbf{P}_i^2 = \\mathbf{P}_i$ for $i=1,\\ldots,k$. That is each $\\mathbf{P}_i$ is a projector.\n",
    "    \n",
    "    Proof: We will prove $1 \\Rightarrow 2 \\Rightarrow 3 \\Rightarrow 1$.  \n",
    "    Proof of $1 \\Rightarrow 2$: Let $r_i = \\text{rank}(\\mathbf{P}_i)$ and $\\mathbf{P}_i = \\mathbf{S}_i \\mathbf{U}_i'$ be a rank factorization of $\\mathbf{P}_i$ for $i=1,\\ldots,k$, where $\\mathbf{S}_i \\in \\mathbb{R}^{n \\times r_i}$ and $\\mathbf{U}_i' \\in \\mathbb{R}^{r_i \\times n}$ with $\\text{rank}(\\mathbf{S}_i) = \\text{rank}(\\mathbf{U}_i) = r_i$. Then \n",
    "\\begin{eqnarray*}\n",
    "    & & \\sum_{i=1}^k \\mathbf{P}_i = \\mathbf{I}_n \\\\\n",
    "    &\\Rightarrow& \\mathbf{S}_1 \\mathbf{U}_1' + \\cdots + \\mathbf{S}_k \\mathbf{U}_k' = \\mathbf{I}_n \\\\\n",
    "    &\\Rightarrow& \\begin{pmatrix} \\mathbf{S}_1 \\, \\cdots \\, \\mathbf{S}_k \\end{pmatrix} \\begin{pmatrix}\n",
    "    \\mathbf{U}_1' \\\\ \\vdots \\\\ \\mathbf{U}'_k \\end{pmatrix} = \\mathbf{I}_n \\\\\n",
    "    &\\Rightarrow& \\mathbf{S} \\mathbf{U}' = \\mathbf{I}_n,\n",
    "\\end{eqnarray*}\n",
    "where\n",
    "$$\n",
    "    \\mathbf{S} = \\begin{pmatrix} \\mathbf{S}_1 \\, \\cdots \\, \\mathbf{S}_k \\end{pmatrix}, \\quad \\mathbf{U}' = \\begin{pmatrix} \\mathbf{U}_1' \\\\ \\vdots \\\\ \\mathbf{U}'_k \\end{pmatrix}.\n",
    "$$\n",
    "Since $\\sum_{i=1}^k r_i = n$, $\\mathbf{S}, \\mathbf{U}$ must be square and $\\mathbf{I}_n = \\mathbf{S} \\mathbf{U}'$ is a rank factorization. $\\mathbf{I}_n$ apparently is idempotent thus we have\n",
    "$$\n",
    "    \\mathbf{U}' \\mathbf{S} = \\mathbf{I}_n = \\begin{pmatrix} \\mathbf{I}_{r_1} & \\cdots & \\mathbf{O} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{O} & \\cdots & \\mathbf{I}_{r_k} \\end{pmatrix}.\n",
    "$$\n",
    "Therefore $\\mathbf{U}_i' \\mathbf{S}_j = \\mathbf{O}$ whenver $i \\ne j$, implying\n",
    "$$\n",
    "    \\mathbf{P}_i \\mathbf{P}_j = \\mathbf{S}_i \\mathbf{U}_i' \\mathbf{S}_j \\mathbf{U}_j' = \\mathbf{O}\n",
    "$$\n",
    "whenever $i \\ne j$.  \n",
    "    Proof of $2 \\Rightarrow 3$: Since $\\sum_{i=1}^k \\mathbf{P}_i = \\mathbf{I}_n$, from 2, \n",
    "$$\n",
    "    \\mathbf{P}_j = \\mathbf{P}_j \\left( \\sum_{i=1}^k \\mathbf{P}_i \\right) = \\mathbf{P}_j^2 + \\sum_{i \\ne j} \\mathbf{P}_i \\mathbf{P}_j = \\mathbf{P}_j^2.\n",
    "$$  \n",
    "    Proof of $3 \\Rightarrow 1$: $\\mathbf{P}_i$ are idemponent, so $\\text{rank}(\\mathbf{P}_i) = \\text{tr}(\\mathbf{P}_i)$ for all $i$. $\\sum_{i=1}^k \\text{rank}(\\mathbf{P}_i) = \\sum_{i=1}^k \\text{tr}(\\mathbf{P}_i) = \\text{tr}(\\sum_i \\mathbf{P}_i) = \\text{tr}(\\mathbf{I}_n) = n$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84.7159px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "467px",
    "left": "0px",
    "right": "1315.28px",
    "top": "33.2px",
    "width": "110.545px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
